{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "03113463-b757-457e-8da3-ee58794b33ee",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dd8159ae",
    "execution_start": 1642363435242,
    "execution_millis": 3050,
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler    \nfrom sklearn.model_selection import train_test_split\nfrom skorch import NeuralNetRegressor\nfrom scipy.stats import uniform\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\nimport torch.nn.functional as F",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00001-0d0173a2-0574-4d5b-b9c7-3549eee93b9a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47b90439",
    "execution_start": 1642363438298,
    "execution_millis": 26,
    "deepnote_cell_type": "code"
   },
   "source": "df_visual = pd.read_csv('./data/ML-CUP21-TR.csv', usecols=[11,12], names=['idx','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10', 'x','y'])\ndf_task_tr = pd.read_csv('./data/ML-CUP21-TR.csv', names=['idx','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10', 'x','y'])\ndf_task_tr = df_task_tr[7:] # remove first 7 rows of comments\ndf_visual = df_visual[7:]\n\ndf_blind_ts = pd.read_csv('./data/ML-CUP21-TS.csv', names=['idx','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10'])\ndf_blind_ts = df_blind_ts[7:]\ndf_blind_ts.drop(labels=\"idx\", axis=1, inplace=True)\nblind_ts = df_blind_ts.values",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-8e3ce472-ff69-4ea7-97e3-d913b512af40",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3afbb4cb",
    "execution_start": 1642363438329,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "def mee(true_target, predicted_target): #assuming target is 2-dim matrix with x and y as columns\n    l = true_target.shape[0]\n    res = 0\n    \n    for p in range(l): #for p-th pattern in l (number of samples)\n        x_diff = np.square(true_target[p,0] - predicted_target[p,0]) #difference between the x value of the true and predicted target\n        y_diff = np.square(true_target[p,1] - predicted_target[p,1]) #difference between the y values of true and predicted target\n        sum_term = x_diff + y_diff\n        res = res + np.sqrt(sum_term)\n    res = res / l\n    \n    #return np.average([np.sqrt(np.square(true_target[p,0] - predicted_target[p,0]) + np.square(true_target[p,1] - predicted_target[p,1])) for p in range(l)]) #compact form\n    return res",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00003-cca68452-1a9f-43f2-a95e-57018eb9a303",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "236d7250",
    "execution_start": 1642363438339,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "def inv_mee(true_target, predicted_target):\n    return 1/mee(true_target, predicted_target)",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-8e65de00-6683-4b7d-9194-4c8ae030aee6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "192dcf",
    "execution_start": 1642363438343,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "df_task_tr.drop(labels=\"idx\", axis=1, inplace=True)",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00005-63853f2c-11dc-4004-9bcb-4e58ba87336d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "adef5ee0",
    "execution_start": 1642363438366,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "y = df_task_tr[['x','y']].values\nX = df_task_tr.drop(labels=['x','y'], axis=1).values",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-9e7e1d91-18d5-4fca-9cb1-70fc39ee71c5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "473a37ed",
    "execution_start": 1642363438366,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "source": "print(X.shape)\nprint(y.shape)",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "(1477, 10)\n(1477, 2)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00007-15a9d6de-0760-442e-896a-cd43db4adcfb",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "596ccc7b",
    "execution_start": 1642363438367,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "RS_NUMBER = 69",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-a49d71c2-f651-41f2-acdc-91530bf94733",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "56f9139b",
    "execution_start": 1642363438371,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "X, X_test, y, y_test = train_test_split(X, y, test_size=0.1, random_state=RS_NUMBER)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS_NUMBER)\nprint(f\"X train shape: {X.shape}\")\nprint(f\"X validation shape: {X_test.shape}\")\nprint(f\"y train shape: {y.shape}\")\nprint(f\"y validation shape: {y_test.shape}\")",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "X train shape: (1329, 10)\nX validation shape: (148, 10)\ny train shape: (1329, 2)\ny validation shape: (148, 2)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00009-9b91a60c-1771-44b7-85d3-4179fd2a3044",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f35c2b20",
    "execution_start": 1642363438382,
    "execution_millis": 7,
    "deepnote_cell_type": "code"
   },
   "source": "class MyModule(nn.Module):\n    def __init__(self, num_units, activation_fun, hidden_layers):\n        super(MyModule, self).__init__()\n        \n        if(activation_fun == \"sigmoid\"):\n            activation_fun = nn.Sigmoid()\n        elif activation_fun == \"relu\":\n            activation_fun = nn.ReLU()\n        elif activation_fun == \"tanh\":\n            activation_fun = nn.Tanh()\n            \n        if hidden_layers == 2:\n            self.linear_stack = nn.Sequential(\n                nn.Linear(10, num_units),\n                activation_fun,\n                nn.Linear(num_units, num_units),\n                activation_fun,\n                nn.Linear(num_units, 2),\n            )\n        elif hidden_layers == 1:\n            self.linear_stack = nn.Sequential(\n                nn.Linear(10, num_units),\n                activation_fun,\n                nn.Linear(num_units, 2),\n            )     \n    def forward(self, x):\n        X = self.linear_stack(x)\n        return X\n    \n    def predict(self, x):\n        X = self.linear_stack(x)\n        return X",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-e23cd2eb-f4f5-4c0c-b985-96731b3e5f4b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fecea97f",
    "execution_start": 1642363438424,
    "execution_millis": 4,
    "deepnote_cell_type": "code"
   },
   "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = MyModule(num_units=100, activation_fun = \"sigmoid\", hidden_layers = 1).to(device)\nprint(model)",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "MyModule(\n  (linear_stack): Sequential(\n    (0): Linear(in_features=10, out_features=100, bias=True)\n    (1): Sigmoid()\n    (2): Linear(in_features=100, out_features=2, bias=True)\n  )\n)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00011-ee26a323-33fd-498d-a315-3a23538a0cf1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6e167acc",
    "execution_start": 1642363438425,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "model = NeuralNetRegressor(\n    module = MyModule\n)",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00012-2fe8f0fe-1ea1-4887-9b7f-061aea6b93a6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e57ac93d",
    "execution_start": 1642363438425,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "# define the number of iterations\nn_iter_search = 100\n# define number of k-folds\nk = 10\nRS_NUMBER = 69",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00013-9807bc36-9a23-4057-a810-b1a8d5997908",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8741b297",
    "execution_start": 1642363438427,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "X = X.astype(np.float32)\ny = y.astype(np.float32)\nX_test = X_test.astype(np.float32)\ny_test = y_test.astype(np.float32)",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00014-f29742e9-defa-4ead-9f7b-1875566e484a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "94233c50",
    "execution_start": 1642363438436,
    "execution_millis": 13,
    "deepnote_cell_type": "code"
   },
   "source": "%%script false --no-raise-error\n\n# to find the best set of parameter setting, we can run a randomized search\n\n# define the parameters' values\nparams = {\n    'lr': uniform(0.0001, 0.1),\n    'max_epochs': randint(50, 500),\n    'optimizer': [torch.optim.SGD],\n    #'criterion': [nn.BCELoss],\n    'optimizer__momentum': [0.8, 0.9, 0.95, 0.85],\n    'optimizer__weight_decay': [0.0001, 0.001, 0.01, 0.1],\n    'optimizer__nesterov': [False, True],\n    'module__num_units': randint(5, 128),\n    'module__hidden_layers': [1,2],\n    'module__activation_fun': [\"sigmoid\", \"tanh\", \"relu\"],\n    'batch_size': randint(1, X.shape[0]),\n}\n\n# define the grid search\nrand_gs = RandomizedSearchCV(\n    model,\n    param_distributions=params,\n    n_iter=n_iter_search,\n    n_jobs=-1,\n    scoring=make_scorer(inv_mee), #scoring is based on the highest value\n    cv=k,\n    random_state=RS_NUMBER,\n    verbose=10\n)\n# run the grid search\nrand_gs.fit(X, y)\n",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00015-c3740584-1ba9-4476-8b79-bfb9a96d2f51",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b977134",
    "execution_start": 1642363438455,
    "execution_millis": 10,
    "deepnote_cell_type": "code"
   },
   "source": "%%script false --no-raise-error\n\ndf_results = pd.DataFrame(data=rand_gs.cv_results_) #trasform into a pandas dataframe\ndf_results.sort_values(by=\"rank_test_score\", axis=0, ascending=True, inplace=True) #sort rank values in decreasing order \ndf_results.dropna(inplace=True) #delete nan values \ndf_results = df_results[:50] #keep only top 50 models\npd.DataFrame.to_csv(df_results, \"pytorch_results/pytorch_search.csv\")",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00016-e7d90d2d-c917-49b9-927c-34cffe186b5a",
    "deepnote_output_heights": [
     208.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4c8f1693",
    "execution_start": 1642363438472,
    "execution_millis": 188,
    "deepnote_cell_type": "code"
   },
   "source": "df_results = pd.read_csv(\"pytorch_results/pytorch_search.csv\")\ndf_results.head()",
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 17,
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 29,
       "row_count": 5,
       "columns": [
        {
         "name": "Unnamed: 0",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "20",
          "max": "58",
          "histogram": [
           {
            "bin_start": 20,
            "bin_end": 23.8,
            "count": 1
           },
           {
            "bin_start": 23.8,
            "bin_end": 27.6,
            "count": 0
           },
           {
            "bin_start": 27.6,
            "bin_end": 31.4,
            "count": 0
           },
           {
            "bin_start": 31.4,
            "bin_end": 35.2,
            "count": 0
           },
           {
            "bin_start": 35.2,
            "bin_end": 39,
            "count": 0
           },
           {
            "bin_start": 39,
            "bin_end": 42.8,
            "count": 0
           },
           {
            "bin_start": 42.8,
            "bin_end": 46.599999999999994,
            "count": 1
           },
           {
            "bin_start": 46.599999999999994,
            "bin_end": 50.4,
            "count": 0
           },
           {
            "bin_start": 50.4,
            "bin_end": 54.199999999999996,
            "count": 1
           },
           {
            "bin_start": 54.199999999999996,
            "bin_end": 58,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "mean_fit_time",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "5.003209090232849",
          "max": "11.609505200386048",
          "histogram": [
           {
            "bin_start": 5.003209090232849,
            "bin_end": 5.663838701248169,
            "count": 2
           },
           {
            "bin_start": 5.663838701248169,
            "bin_end": 6.324468312263488,
            "count": 0
           },
           {
            "bin_start": 6.324468312263488,
            "bin_end": 6.985097923278809,
            "count": 0
           },
           {
            "bin_start": 6.985097923278809,
            "bin_end": 7.645727534294128,
            "count": 0
           },
           {
            "bin_start": 7.645727534294128,
            "bin_end": 8.306357145309448,
            "count": 0
           },
           {
            "bin_start": 8.306357145309448,
            "bin_end": 8.966986756324768,
            "count": 1
           },
           {
            "bin_start": 8.966986756324768,
            "bin_end": 9.627616367340089,
            "count": 0
           },
           {
            "bin_start": 9.627616367340089,
            "bin_end": 10.288245978355409,
            "count": 1
           },
           {
            "bin_start": 10.288245978355409,
            "bin_end": 10.94887558937073,
            "count": 0
           },
           {
            "bin_start": 10.94887558937073,
            "bin_end": 11.609505200386048,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "std_fit_time",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.118702509217867",
          "max": "0.3532210056773126",
          "histogram": [
           {
            "bin_start": 0.118702509217867,
            "bin_end": 0.14215435886381156,
            "count": 1
           },
           {
            "bin_start": 0.14215435886381156,
            "bin_end": 0.16560620850975613,
            "count": 0
           },
           {
            "bin_start": 0.16560620850975613,
            "bin_end": 0.1890580581557007,
            "count": 0
           },
           {
            "bin_start": 0.1890580581557007,
            "bin_end": 0.21250990780164525,
            "count": 0
           },
           {
            "bin_start": 0.21250990780164525,
            "bin_end": 0.2359617574475898,
            "count": 0
           },
           {
            "bin_start": 0.2359617574475898,
            "bin_end": 0.2594136070935344,
            "count": 0
           },
           {
            "bin_start": 0.2594136070935344,
            "bin_end": 0.28286545673947894,
            "count": 1
           },
           {
            "bin_start": 0.28286545673947894,
            "bin_end": 0.3063173063854235,
            "count": 1
           },
           {
            "bin_start": 0.3063173063854235,
            "bin_end": 0.3297691560313681,
            "count": 0
           },
           {
            "bin_start": 0.3297691560313681,
            "bin_end": 0.3532210056773126,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "mean_score_time",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.0039577960968017",
          "max": "0.0043504238128662",
          "histogram": [
           {
            "bin_start": 0.0039577960968017,
            "bin_end": 0.00399705886840815,
            "count": 1
           },
           {
            "bin_start": 0.00399705886840815,
            "bin_end": 0.0040363216400146,
            "count": 1
           },
           {
            "bin_start": 0.0040363216400146,
            "bin_end": 0.00407558441162105,
            "count": 0
           },
           {
            "bin_start": 0.00407558441162105,
            "bin_end": 0.0041148471832275,
            "count": 0
           },
           {
            "bin_start": 0.0041148471832275,
            "bin_end": 0.004154109954833949,
            "count": 1
           },
           {
            "bin_start": 0.004154109954833949,
            "bin_end": 0.0041933727264404,
            "count": 1
           },
           {
            "bin_start": 0.0041933727264404,
            "bin_end": 0.00423263549804685,
            "count": 0
           },
           {
            "bin_start": 0.00423263549804685,
            "bin_end": 0.004271898269653299,
            "count": 0
           },
           {
            "bin_start": 0.004271898269653299,
            "bin_end": 0.00431116104125975,
            "count": 0
           },
           {
            "bin_start": 0.00431116104125975,
            "bin_end": 0.0043504238128662,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "std_score_time",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.0010232031628521",
          "max": "0.0019124797944501",
          "histogram": [
           {
            "bin_start": 0.0010232031628521,
            "bin_end": 0.0011121308260119,
            "count": 2
           },
           {
            "bin_start": 0.0011121308260119,
            "bin_end": 0.0012010584891717,
            "count": 1
           },
           {
            "bin_start": 0.0012010584891717,
            "bin_end": 0.0012899861523315,
            "count": 0
           },
           {
            "bin_start": 0.0012899861523315,
            "bin_end": 0.0013789138154913,
            "count": 1
           },
           {
            "bin_start": 0.0013789138154913,
            "bin_end": 0.0014678414786511,
            "count": 0
           },
           {
            "bin_start": 0.0014678414786511,
            "bin_end": 0.0015567691418109,
            "count": 0
           },
           {
            "bin_start": 0.0015567691418109,
            "bin_end": 0.0016456968049707,
            "count": 0
           },
           {
            "bin_start": 0.0016456968049707,
            "bin_end": 0.0017346244681305,
            "count": 0
           },
           {
            "bin_start": 0.0017346244681305,
            "bin_end": 0.0018235521312903,
            "count": 0
           },
           {
            "bin_start": 0.0018235521312903,
            "bin_end": 0.0019124797944501,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "param_batch_size",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "304",
          "max": "1227",
          "histogram": [
           {
            "bin_start": 304,
            "bin_end": 396.3,
            "count": 1
           },
           {
            "bin_start": 396.3,
            "bin_end": 488.6,
            "count": 1
           },
           {
            "bin_start": 488.6,
            "bin_end": 580.9,
            "count": 1
           },
           {
            "bin_start": 580.9,
            "bin_end": 673.2,
            "count": 0
           },
           {
            "bin_start": 673.2,
            "bin_end": 765.5,
            "count": 0
           },
           {
            "bin_start": 765.5,
            "bin_end": 857.8,
            "count": 0
           },
           {
            "bin_start": 857.8,
            "bin_end": 950.1,
            "count": 0
           },
           {
            "bin_start": 950.1,
            "bin_end": 1042.4,
            "count": 0
           },
           {
            "bin_start": 1042.4,
            "bin_end": 1134.6999999999998,
            "count": 1
           },
           {
            "bin_start": 1134.6999999999998,
            "bin_end": 1227,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "param_lr",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.0031991330033534",
          "max": "0.0238821335641144",
          "histogram": [
           {
            "bin_start": 0.0031991330033534,
            "bin_end": 0.0052674330594295,
            "count": 2
           },
           {
            "bin_start": 0.0052674330594295,
            "bin_end": 0.0073357331155055995,
            "count": 1
           },
           {
            "bin_start": 0.0073357331155055995,
            "bin_end": 0.0094040331715817,
            "count": 0
           },
           {
            "bin_start": 0.0094040331715817,
            "bin_end": 0.0114723332276578,
            "count": 0
           },
           {
            "bin_start": 0.0114723332276578,
            "bin_end": 0.0135406332837339,
            "count": 0
           },
           {
            "bin_start": 0.0135406332837339,
            "bin_end": 0.015608933339809998,
            "count": 0
           },
           {
            "bin_start": 0.015608933339809998,
            "bin_end": 0.0176772333958861,
            "count": 0
           },
           {
            "bin_start": 0.0176772333958861,
            "bin_end": 0.0197455334519622,
            "count": 0
           },
           {
            "bin_start": 0.0197455334519622,
            "bin_end": 0.0218138335080383,
            "count": 0
           },
           {
            "bin_start": 0.0218138335080383,
            "bin_end": 0.0238821335641144,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "param_max_epochs",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "231",
          "max": "441",
          "histogram": [
           {
            "bin_start": 231,
            "bin_end": 252,
            "count": 2
           },
           {
            "bin_start": 252,
            "bin_end": 273,
            "count": 0
           },
           {
            "bin_start": 273,
            "bin_end": 294,
            "count": 0
           },
           {
            "bin_start": 294,
            "bin_end": 315,
            "count": 0
           },
           {
            "bin_start": 315,
            "bin_end": 336,
            "count": 0
           },
           {
            "bin_start": 336,
            "bin_end": 357,
            "count": 0
           },
           {
            "bin_start": 357,
            "bin_end": 378,
            "count": 0
           },
           {
            "bin_start": 378,
            "bin_end": 399,
            "count": 0
           },
           {
            "bin_start": 399,
            "bin_end": 420,
            "count": 0
           },
           {
            "bin_start": 420,
            "bin_end": 441,
            "count": 3
           }
          ]
         }
        },
        {
         "name": "param_module__activation_fun",
         "dtype": "object",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "categories": [
           {
            "name": "tanh",
            "count": 4
           },
           {
            "name": "sigmoid",
            "count": 1
           }
          ]
         }
        },
        {
         "name": "param_module__hidden_layers",
         "dtype": "int64",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "min": "1",
          "max": "2",
          "histogram": [
           {
            "bin_start": 1,
            "bin_end": 1.1,
            "count": 3
           },
           {
            "bin_start": 1.1,
            "bin_end": 1.2,
            "count": 0
           },
           {
            "bin_start": 1.2,
            "bin_end": 1.3,
            "count": 0
           },
           {
            "bin_start": 1.3,
            "bin_end": 1.4,
            "count": 0
           },
           {
            "bin_start": 1.4,
            "bin_end": 1.5,
            "count": 0
           },
           {
            "bin_start": 1.5,
            "bin_end": 1.6,
            "count": 0
           },
           {
            "bin_start": 1.6,
            "bin_end": 1.7000000000000002,
            "count": 0
           },
           {
            "bin_start": 1.7000000000000002,
            "bin_end": 1.8,
            "count": 0
           },
           {
            "bin_start": 1.8,
            "bin_end": 1.9,
            "count": 0
           },
           {
            "bin_start": 1.9,
            "bin_end": 2,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "param_module__num_units",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "51",
          "max": "113",
          "histogram": [
           {
            "bin_start": 51,
            "bin_end": 57.2,
            "count": 1
           },
           {
            "bin_start": 57.2,
            "bin_end": 63.4,
            "count": 0
           },
           {
            "bin_start": 63.4,
            "bin_end": 69.6,
            "count": 0
           },
           {
            "bin_start": 69.6,
            "bin_end": 75.8,
            "count": 1
           },
           {
            "bin_start": 75.8,
            "bin_end": 82,
            "count": 1
           },
           {
            "bin_start": 82,
            "bin_end": 88.2,
            "count": 0
           },
           {
            "bin_start": 88.2,
            "bin_end": 94.4,
            "count": 0
           },
           {
            "bin_start": 94.4,
            "bin_end": 100.6,
            "count": 0
           },
           {
            "bin_start": 100.6,
            "bin_end": 106.80000000000001,
            "count": 1
           },
           {
            "bin_start": 106.80000000000001,
            "bin_end": 113,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "param_optimizer",
         "dtype": "object",
         "stats": {
          "unique_count": 1,
          "nan_count": 0,
          "categories": [
           {
            "name": "<class 'torch.optim.sgd.SGD'>",
            "count": 5
           }
          ]
         }
        },
        {
         "name": "param_optimizer__momentum",
         "dtype": "float64",
         "stats": {
          "unique_count": 3,
          "nan_count": 0,
          "min": "0.85",
          "max": "0.95",
          "histogram": [
           {
            "bin_start": 0.85,
            "bin_end": 0.86,
            "count": 2
           },
           {
            "bin_start": 0.86,
            "bin_end": 0.87,
            "count": 0
           },
           {
            "bin_start": 0.87,
            "bin_end": 0.88,
            "count": 0
           },
           {
            "bin_start": 0.88,
            "bin_end": 0.89,
            "count": 0
           },
           {
            "bin_start": 0.89,
            "bin_end": 0.8999999999999999,
            "count": 0
           },
           {
            "bin_start": 0.8999999999999999,
            "bin_end": 0.9099999999999999,
            "count": 1
           },
           {
            "bin_start": 0.9099999999999999,
            "bin_end": 0.9199999999999999,
            "count": 0
           },
           {
            "bin_start": 0.9199999999999999,
            "bin_end": 0.9299999999999999,
            "count": 0
           },
           {
            "bin_start": 0.9299999999999999,
            "bin_end": 0.94,
            "count": 0
           },
           {
            "bin_start": 0.94,
            "bin_end": 0.95,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "param_optimizer__nesterov",
         "dtype": "bool",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "categories": [
           {
            "name": "False",
            "count": 4
           },
           {
            "name": "True",
            "count": 1
           }
          ]
         }
        },
        {
         "name": "param_optimizer__weight_decay",
         "dtype": "float64",
         "stats": {
          "unique_count": 3,
          "nan_count": 0,
          "min": "0.0001",
          "max": "0.01",
          "histogram": [
           {
            "bin_start": 0.0001,
            "bin_end": 0.00109,
            "count": 4
           },
           {
            "bin_start": 0.00109,
            "bin_end": 0.00208,
            "count": 0
           },
           {
            "bin_start": 0.00208,
            "bin_end": 0.00307,
            "count": 0
           },
           {
            "bin_start": 0.00307,
            "bin_end": 0.00406,
            "count": 0
           },
           {
            "bin_start": 0.00406,
            "bin_end": 0.00505,
            "count": 0
           },
           {
            "bin_start": 0.00505,
            "bin_end": 0.00604,
            "count": 0
           },
           {
            "bin_start": 0.00604,
            "bin_end": 0.007030000000000001,
            "count": 0
           },
           {
            "bin_start": 0.007030000000000001,
            "bin_end": 0.00802,
            "count": 0
           },
           {
            "bin_start": 0.00802,
            "bin_end": 0.009009999999999999,
            "count": 0
           },
           {
            "bin_start": 0.009009999999999999,
            "bin_end": 0.01,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "params",
         "dtype": "object",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "categories": [
           {
            "name": "{'batch_size': 525, 'lr': 0.02388213356411444, 'max_epochs': 231, 'module__activation_fun': 'sigmoid', 'module__hidden_layers': 1, 'module__num_units': 51, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'optimizer__momentum': 0.95, 'optimizer__nesterov': False, 'optimizer__weight_decay': 0.001}",
            "count": 1
           },
           {
            "name": "{'batch_size': 304, 'lr': 0.0031991330033534935, 'max_epochs': 432, 'module__activation_fun': 'tanh', 'module__hidden_layers': 2, 'module__num_units': 79, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'optimizer__momentum': 0.85, 'optimizer__nesterov': False, 'optimizer__weight_decay': 0.0001}",
            "count": 1
           },
           {
            "name": "3 others",
            "count": 3
           }
          ]
         }
        },
        {
         "name": "split0_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.9208238969520178",
          "max": "0.9489776510449192",
          "histogram": [
           {
            "bin_start": 0.9208238969520178,
            "bin_end": 0.9236392723613079,
            "count": 1
           },
           {
            "bin_start": 0.9236392723613079,
            "bin_end": 0.926454647770598,
            "count": 0
           },
           {
            "bin_start": 0.926454647770598,
            "bin_end": 0.9292700231798883,
            "count": 1
           },
           {
            "bin_start": 0.9292700231798883,
            "bin_end": 0.9320853985891784,
            "count": 1
           },
           {
            "bin_start": 0.9320853985891784,
            "bin_end": 0.9349007739984685,
            "count": 0
           },
           {
            "bin_start": 0.9349007739984685,
            "bin_end": 0.9377161494077586,
            "count": 1
           },
           {
            "bin_start": 0.9377161494077586,
            "bin_end": 0.9405315248170487,
            "count": 0
           },
           {
            "bin_start": 0.9405315248170487,
            "bin_end": 0.9433469002263389,
            "count": 0
           },
           {
            "bin_start": 0.9433469002263389,
            "bin_end": 0.946162275635629,
            "count": 0
           },
           {
            "bin_start": 0.946162275635629,
            "bin_end": 0.9489776510449192,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "split1_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.917979185961768",
          "max": "0.9683618647815296",
          "histogram": [
           {
            "bin_start": 0.917979185961768,
            "bin_end": 0.9230174538437441,
            "count": 1
           },
           {
            "bin_start": 0.9230174538437441,
            "bin_end": 0.9280557217257203,
            "count": 0
           },
           {
            "bin_start": 0.9280557217257203,
            "bin_end": 0.9330939896076965,
            "count": 0
           },
           {
            "bin_start": 0.9330939896076965,
            "bin_end": 0.9381322574896727,
            "count": 1
           },
           {
            "bin_start": 0.9381322574896727,
            "bin_end": 0.9431705253716488,
            "count": 0
           },
           {
            "bin_start": 0.9431705253716488,
            "bin_end": 0.948208793253625,
            "count": 1
           },
           {
            "bin_start": 0.948208793253625,
            "bin_end": 0.9532470611356011,
            "count": 1
           },
           {
            "bin_start": 0.9532470611356011,
            "bin_end": 0.9582853290175773,
            "count": 0
           },
           {
            "bin_start": 0.9582853290175773,
            "bin_end": 0.9633235968995535,
            "count": 0
           },
           {
            "bin_start": 0.9633235968995535,
            "bin_end": 0.9683618647815296,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "split2_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.822140977320871",
          "max": "0.8585147300282389",
          "histogram": [
           {
            "bin_start": 0.822140977320871,
            "bin_end": 0.8257783525916078,
            "count": 1
           },
           {
            "bin_start": 0.8257783525916078,
            "bin_end": 0.8294157278623445,
            "count": 0
           },
           {
            "bin_start": 0.8294157278623445,
            "bin_end": 0.8330531031330813,
            "count": 1
           },
           {
            "bin_start": 0.8330531031330813,
            "bin_end": 0.8366904784038182,
            "count": 0
           },
           {
            "bin_start": 0.8366904784038182,
            "bin_end": 0.8403278536745549,
            "count": 0
           },
           {
            "bin_start": 0.8403278536745549,
            "bin_end": 0.8439652289452917,
            "count": 1
           },
           {
            "bin_start": 0.8439652289452917,
            "bin_end": 0.8476026042160285,
            "count": 0
           },
           {
            "bin_start": 0.8476026042160285,
            "bin_end": 0.8512399794867653,
            "count": 0
           },
           {
            "bin_start": 0.8512399794867653,
            "bin_end": 0.854877354757502,
            "count": 1
           },
           {
            "bin_start": 0.854877354757502,
            "bin_end": 0.8585147300282389,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "split3_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.7957580889657434",
          "max": "0.8299022217471078",
          "histogram": [
           {
            "bin_start": 0.7957580889657434,
            "bin_end": 0.7991725022438798,
            "count": 2
           },
           {
            "bin_start": 0.7991725022438798,
            "bin_end": 0.8025869155220163,
            "count": 0
           },
           {
            "bin_start": 0.8025869155220163,
            "bin_end": 0.8060013288001527,
            "count": 0
           },
           {
            "bin_start": 0.8060013288001527,
            "bin_end": 0.8094157420782891,
            "count": 0
           },
           {
            "bin_start": 0.8094157420782891,
            "bin_end": 0.8128301553564257,
            "count": 0
           },
           {
            "bin_start": 0.8128301553564257,
            "bin_end": 0.8162445686345621,
            "count": 1
           },
           {
            "bin_start": 0.8162445686345621,
            "bin_end": 0.8196589819126985,
            "count": 1
           },
           {
            "bin_start": 0.8196589819126985,
            "bin_end": 0.8230733951908349,
            "count": 0
           },
           {
            "bin_start": 0.8230733951908349,
            "bin_end": 0.8264878084689714,
            "count": 0
           },
           {
            "bin_start": 0.8264878084689714,
            "bin_end": 0.8299022217471078,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "split4_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.7999432952104479",
          "max": "0.8245343251175447",
          "histogram": [
           {
            "bin_start": 0.7999432952104479,
            "bin_end": 0.8024023982011577,
            "count": 1
           },
           {
            "bin_start": 0.8024023982011577,
            "bin_end": 0.8048615011918673,
            "count": 0
           },
           {
            "bin_start": 0.8048615011918673,
            "bin_end": 0.8073206041825769,
            "count": 0
           },
           {
            "bin_start": 0.8073206041825769,
            "bin_end": 0.8097797071732866,
            "count": 1
           },
           {
            "bin_start": 0.8097797071732866,
            "bin_end": 0.8122388101639963,
            "count": 1
           },
           {
            "bin_start": 0.8122388101639963,
            "bin_end": 0.814697913154706,
            "count": 0
           },
           {
            "bin_start": 0.814697913154706,
            "bin_end": 0.8171570161454156,
            "count": 1
           },
           {
            "bin_start": 0.8171570161454156,
            "bin_end": 0.8196161191361253,
            "count": 0
           },
           {
            "bin_start": 0.8196161191361253,
            "bin_end": 0.822075222126835,
            "count": 0
           },
           {
            "bin_start": 0.822075222126835,
            "bin_end": 0.8245343251175447,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "split5_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.7966047935005637",
          "max": "0.8120701065714183",
          "histogram": [
           {
            "bin_start": 0.7966047935005637,
            "bin_end": 0.7981513248076492,
            "count": 1
           },
           {
            "bin_start": 0.7981513248076492,
            "bin_end": 0.7996978561147347,
            "count": 0
           },
           {
            "bin_start": 0.7996978561147347,
            "bin_end": 0.80124438742182,
            "count": 1
           },
           {
            "bin_start": 0.80124438742182,
            "bin_end": 0.8027909187289055,
            "count": 1
           },
           {
            "bin_start": 0.8027909187289055,
            "bin_end": 0.804337450035991,
            "count": 0
           },
           {
            "bin_start": 0.804337450035991,
            "bin_end": 0.8058839813430765,
            "count": 0
           },
           {
            "bin_start": 0.8058839813430765,
            "bin_end": 0.807430512650162,
            "count": 1
           },
           {
            "bin_start": 0.807430512650162,
            "bin_end": 0.8089770439572473,
            "count": 0
           },
           {
            "bin_start": 0.8089770439572473,
            "bin_end": 0.8105235752643328,
            "count": 0
           },
           {
            "bin_start": 0.8105235752643328,
            "bin_end": 0.8120701065714183,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "split6_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.8532892358201447",
          "max": "0.8886620454499092",
          "histogram": [
           {
            "bin_start": 0.8532892358201447,
            "bin_end": 0.8568265167831212,
            "count": 1
           },
           {
            "bin_start": 0.8568265167831212,
            "bin_end": 0.8603637977460976,
            "count": 0
           },
           {
            "bin_start": 0.8603637977460976,
            "bin_end": 0.8639010787090741,
            "count": 0
           },
           {
            "bin_start": 0.8639010787090741,
            "bin_end": 0.8674383596720505,
            "count": 0
           },
           {
            "bin_start": 0.8674383596720505,
            "bin_end": 0.870975640635027,
            "count": 1
           },
           {
            "bin_start": 0.870975640635027,
            "bin_end": 0.8745129215980034,
            "count": 0
           },
           {
            "bin_start": 0.8745129215980034,
            "bin_end": 0.8780502025609799,
            "count": 0
           },
           {
            "bin_start": 0.8780502025609799,
            "bin_end": 0.8815874835239563,
            "count": 1
           },
           {
            "bin_start": 0.8815874835239563,
            "bin_end": 0.8851247644869328,
            "count": 1
           },
           {
            "bin_start": 0.8851247644869328,
            "bin_end": 0.8886620454499092,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "split7_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.9249375030296848",
          "max": "0.9468101029707534",
          "histogram": [
           {
            "bin_start": 0.9249375030296848,
            "bin_end": 0.9271247630237917,
            "count": 1
           },
           {
            "bin_start": 0.9271247630237917,
            "bin_end": 0.9293120230178985,
            "count": 2
           },
           {
            "bin_start": 0.9293120230178985,
            "bin_end": 0.9314992830120054,
            "count": 0
           },
           {
            "bin_start": 0.9314992830120054,
            "bin_end": 0.9336865430061122,
            "count": 0
           },
           {
            "bin_start": 0.9336865430061122,
            "bin_end": 0.9358738030002192,
            "count": 0
           },
           {
            "bin_start": 0.9358738030002192,
            "bin_end": 0.938061062994326,
            "count": 0
           },
           {
            "bin_start": 0.938061062994326,
            "bin_end": 0.9402483229884329,
            "count": 0
           },
           {
            "bin_start": 0.9402483229884329,
            "bin_end": 0.9424355829825397,
            "count": 0
           },
           {
            "bin_start": 0.9424355829825397,
            "bin_end": 0.9446228429766466,
            "count": 0
           },
           {
            "bin_start": 0.9446228429766466,
            "bin_end": 0.9468101029707534,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "split8_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.7705785485813509",
          "max": "0.8091516557437369",
          "histogram": [
           {
            "bin_start": 0.7705785485813509,
            "bin_end": 0.7744358592975895,
            "count": 1
           },
           {
            "bin_start": 0.7744358592975895,
            "bin_end": 0.7782931700138281,
            "count": 0
           },
           {
            "bin_start": 0.7782931700138281,
            "bin_end": 0.7821504807300667,
            "count": 0
           },
           {
            "bin_start": 0.7821504807300667,
            "bin_end": 0.7860077914463053,
            "count": 1
           },
           {
            "bin_start": 0.7860077914463053,
            "bin_end": 0.7898651021625439,
            "count": 0
           },
           {
            "bin_start": 0.7898651021625439,
            "bin_end": 0.7937224128787825,
            "count": 1
           },
           {
            "bin_start": 0.7937224128787825,
            "bin_end": 0.7975797235950212,
            "count": 0
           },
           {
            "bin_start": 0.7975797235950212,
            "bin_end": 0.8014370343112597,
            "count": 0
           },
           {
            "bin_start": 0.8014370343112597,
            "bin_end": 0.8052943450274983,
            "count": 0
           },
           {
            "bin_start": 0.8052943450274983,
            "bin_end": 0.8091516557437369,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "split9_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.9077263985913288",
          "max": "0.981634082875193",
          "histogram": [
           {
            "bin_start": 0.9077263985913288,
            "bin_end": 0.9151171670197151,
            "count": 1
           },
           {
            "bin_start": 0.9151171670197151,
            "bin_end": 0.9225079354481016,
            "count": 0
           },
           {
            "bin_start": 0.9225079354481016,
            "bin_end": 0.9298987038764881,
            "count": 1
           },
           {
            "bin_start": 0.9298987038764881,
            "bin_end": 0.9372894723048745,
            "count": 0
           },
           {
            "bin_start": 0.9372894723048745,
            "bin_end": 0.9446802407332608,
            "count": 1
           },
           {
            "bin_start": 0.9446802407332608,
            "bin_end": 0.9520710091616473,
            "count": 0
           },
           {
            "bin_start": 0.9520710091616473,
            "bin_end": 0.9594617775900338,
            "count": 0
           },
           {
            "bin_start": 0.9594617775900338,
            "bin_end": 0.9668525460184202,
            "count": 0
           },
           {
            "bin_start": 0.9668525460184202,
            "bin_end": 0.9742433144468066,
            "count": 1
           },
           {
            "bin_start": 0.9742433144468066,
            "bin_end": 0.981634082875193,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "mean_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.8619209505993956",
          "max": "0.8760309958431405",
          "histogram": [
           {
            "bin_start": 0.8619209505993956,
            "bin_end": 0.86333195512377,
            "count": 1
           },
           {
            "bin_start": 0.86333195512377,
            "bin_end": 0.8647429596481445,
            "count": 0
           },
           {
            "bin_start": 0.8647429596481445,
            "bin_end": 0.866153964172519,
            "count": 0
           },
           {
            "bin_start": 0.866153964172519,
            "bin_end": 0.8675649686968935,
            "count": 1
           },
           {
            "bin_start": 0.8675649686968935,
            "bin_end": 0.8689759732212681,
            "count": 1
           },
           {
            "bin_start": 0.8689759732212681,
            "bin_end": 0.8703869777456426,
            "count": 0
           },
           {
            "bin_start": 0.8703869777456426,
            "bin_end": 0.871797982270017,
            "count": 0
           },
           {
            "bin_start": 0.871797982270017,
            "bin_end": 0.8732089867943915,
            "count": 1
           },
           {
            "bin_start": 0.8732089867943915,
            "bin_end": 0.874619991318766,
            "count": 0
           },
           {
            "bin_start": 0.874619991318766,
            "bin_end": 0.8760309958431405,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "std_test_score",
         "dtype": "float64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "0.0567760431020931",
          "max": "0.0667477468940614",
          "histogram": [
           {
            "bin_start": 0.0567760431020931,
            "bin_end": 0.057773213481289935,
            "count": 1
           },
           {
            "bin_start": 0.057773213481289935,
            "bin_end": 0.05877038386048676,
            "count": 0
           },
           {
            "bin_start": 0.05877038386048676,
            "bin_end": 0.05976755423968359,
            "count": 0
           },
           {
            "bin_start": 0.05976755423968359,
            "bin_end": 0.060764724618880425,
            "count": 0
           },
           {
            "bin_start": 0.060764724618880425,
            "bin_end": 0.06176189499807726,
            "count": 0
           },
           {
            "bin_start": 0.06176189499807726,
            "bin_end": 0.06275906537727409,
            "count": 2
           },
           {
            "bin_start": 0.06275906537727409,
            "bin_end": 0.06375623575647091,
            "count": 0
           },
           {
            "bin_start": 0.06375623575647091,
            "bin_end": 0.06475340613566774,
            "count": 1
           },
           {
            "bin_start": 0.06475340613566774,
            "bin_end": 0.06575057651486457,
            "count": 0
           },
           {
            "bin_start": 0.06575057651486457,
            "bin_end": 0.0667477468940614,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "rank_test_score",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": "1",
          "max": "5",
          "histogram": [
           {
            "bin_start": 1,
            "bin_end": 1.4,
            "count": 1
           },
           {
            "bin_start": 1.4,
            "bin_end": 1.8,
            "count": 0
           },
           {
            "bin_start": 1.8,
            "bin_end": 2.2,
            "count": 1
           },
           {
            "bin_start": 2.2,
            "bin_end": 2.6,
            "count": 0
           },
           {
            "bin_start": 2.6,
            "bin_end": 3,
            "count": 0
           },
           {
            "bin_start": 3,
            "bin_end": 3.4000000000000004,
            "count": 1
           },
           {
            "bin_start": 3.4000000000000004,
            "bin_end": 3.8000000000000003,
            "count": 0
           },
           {
            "bin_start": 3.8000000000000003,
            "bin_end": 4.2,
            "count": 1
           },
           {
            "bin_start": 4.2,
            "bin_end": 4.6,
            "count": 0
           },
           {
            "bin_start": 4.6,
            "bin_end": 5,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows": [
        {
         "Unnamed: 0": 52,
         "mean_fit_time": 5.003209090232849,
         "std_fit_time": 0.118702509217867,
         "mean_score_time": 0.0043504238128662,
         "std_score_time": 0.0011124580293736,
         "param_batch_size": 525,
         "param_lr": 0.0238821335641144,
         "param_max_epochs": 231,
         "param_module__activation_fun": "sigmoid",
         "param_module__hidden_layers": 1,
         "param_module__num_units": 51,
         "param_optimizer": "<class 'torch.optim.sgd.SGD'>",
         "param_optimizer__momentum": 0.95,
         "param_optimizer__nesterov": false,
         "param_optimizer__weight_decay": 0.001,
         "params": "{'batch_size': 525, 'lr': 0.02388213356411444, 'max_epochs': 231, 'module__activation_fun': 'sigmoi…",
         "split0_test_score": 0.9286189981854088,
         "split1_test_score": 0.9511604389853768,
         "split2_test_score": 0.8585147300282389,
         "split3_test_score": 0.7963052446009549,
         "split4_test_score": 0.8245343251175447,
         "split5_test_score": 0.8007936891136191,
         "split6_test_score": 0.8846592907516475,
         "split7_test_score": 0.9249375030296848,
         "split8_test_score": 0.8091516557437369,
         "split9_test_score": 0.981634082875193,
         "mean_test_score": 0.8760309958431405,
         "std_test_score": 0.0644023739137162,
         "rank_test_score": 1,
         "_deepnote_index_column": 0
        },
        {
         "Unnamed: 0": 57,
         "mean_fit_time": 11.609505200386048,
         "std_fit_time": 0.3460651276157243,
         "mean_score_time": 0.004026460647583,
         "std_score_time": 0.0010232031628521,
         "param_batch_size": 304,
         "param_lr": 0.0031991330033534,
         "param_max_epochs": 432,
         "param_module__activation_fun": "tanh",
         "param_module__hidden_layers": 2,
         "param_module__num_units": 79,
         "param_optimizer": "<class 'torch.optim.sgd.SGD'>",
         "param_optimizer__momentum": 0.85,
         "param_optimizer__nesterov": false,
         "param_optimizer__weight_decay": 0.0001,
         "params": "{'batch_size': 304, 'lr': 0.0031991330033534935, 'max_epochs': 432, 'module__activation_fun': 'tanh…",
         "split0_test_score": 0.9357293801208284,
         "split1_test_score": 0.9443500227325548,
         "split2_test_score": 0.8318912833961893,
         "split3_test_score": 0.8299022217471078,
         "split4_test_score": 0.7999432952104479,
         "split5_test_score": 0.8062058835706121,
         "split6_test_score": 0.881459627463779,
         "split7_test_score": 0.9448213522292054,
         "split8_test_score": 0.7855252644877488,
         "split9_test_score": 0.9694317819088716,
         "mean_test_score": 0.8729260112867345,
         "std_test_score": 0.0667477468940614,
         "rank_test_score": 2,
         "_deepnote_index_column": 1
        },
        {
         "Unnamed: 0": 43,
         "mean_fit_time": 8.331176066398621,
         "std_fit_time": 0.2923452978640132,
         "mean_score_time": 0.0039577960968017,
         "std_score_time": 0.0013164901012843,
         "param_batch_size": 1121,
         "param_lr": 0.0229581892901476,
         "param_max_epochs": 429,
         "param_module__activation_fun": "tanh",
         "param_module__hidden_layers": 1,
         "param_module__num_units": 70,
         "param_optimizer": "<class 'torch.optim.sgd.SGD'>",
         "param_optimizer__momentum": 0.85,
         "param_optimizer__nesterov": false,
         "param_optimizer__weight_decay": 0.001,
         "params": "{'batch_size': 1121, 'lr': 0.0229581892901476, 'max_epochs': 429, 'module__activation_fun': 'tanh',…",
         "split0_test_score": 0.9489776510449192,
         "split1_test_score": 0.9683618647815296,
         "split2_test_score": 0.8412286289524793,
         "split3_test_score": 0.8162759561678604,
         "split4_test_score": 0.8087144838806931,
         "split5_test_score": 0.8024330788334764,
         "split6_test_score": 0.869785523761946,
         "split7_test_score": 0.9277567062627736,
         "split8_test_score": 0.7933078813425954,
         "split9_test_score": 0.9077263985913288,
         "mean_test_score": 0.8684568173619602,
         "std_test_score": 0.0621005837445257,
         "rank_test_score": 3,
         "_deepnote_index_column": 2
        },
        {
         "Unnamed: 0": 20,
         "mean_fit_time": 10.215749502182009,
         "std_fit_time": 0.3532210056773126,
         "mean_score_time": 0.0041551351547241,
         "std_score_time": 0.0010305960023197,
         "param_batch_size": 1227,
         "param_lr": 0.006878206723193,
         "param_max_epochs": 441,
         "param_module__activation_fun": "tanh",
         "param_module__hidden_layers": 2,
         "param_module__num_units": 102,
         "param_optimizer": "<class 'torch.optim.sgd.SGD'>",
         "param_optimizer__momentum": 0.9,
         "param_optimizer__nesterov": true,
         "param_optimizer__weight_decay": 0.01,
         "params": "{'batch_size': 1227, 'lr': 0.006878206723193037, 'max_epochs': 441, 'module__activation_fun': 'tanh…",
         "split0_test_score": 0.9208238969520178,
         "split1_test_score": 0.9379768876225644,
         "split2_test_score": 0.8533459510826892,
         "split3_test_score": 0.8130896171539002,
         "split4_test_score": 0.8156578322974459,
         "split5_test_score": 0.7966047935005637,
         "split6_test_score": 0.8532892358201447,
         "split7_test_score": 0.9280015162096918,
         "split8_test_score": 0.8054413735758754,
         "split9_test_score": 0.9439278814381092,
         "mean_test_score": 0.8668158985653003,
         "std_test_score": 0.0567760431020931,
         "rank_test_score": 4,
         "_deepnote_index_column": 3
        },
        {
         "Unnamed: 0": 58,
         "mean_fit_time": 5.581401252746582,
         "std_fit_time": 0.2694101765560845,
         "mean_score_time": 0.0041510343551635,
         "std_score_time": 0.0019124797944501,
         "param_batch_size": 405,
         "param_lr": 0.0034872183136935,
         "param_max_epochs": 244,
         "param_module__activation_fun": "tanh",
         "param_module__hidden_layers": 1,
         "param_module__num_units": 113,
         "param_optimizer": "<class 'torch.optim.sgd.SGD'>",
         "param_optimizer__momentum": 0.95,
         "param_optimizer__nesterov": false,
         "param_optimizer__weight_decay": 0.0001,
         "params": "{'batch_size': 405, 'lr': 0.003487218313693563, 'max_epochs': 244, 'module__activation_fun': 'tanh'…",
         "split0_test_score": 0.9297826513432788,
         "split1_test_score": 0.917979185961768,
         "split2_test_score": 0.822140977320871,
         "split3_test_score": 0.7957580889657434,
         "split4_test_score": 0.8117106035650372,
         "split5_test_score": 0.8120701065714183,
         "split6_test_score": 0.8886620454499092,
         "split7_test_score": 0.9468101029707534,
         "split8_test_score": 0.7705785485813509,
         "split9_test_score": 0.9237171952638252,
         "mean_test_score": 0.8619209505993956,
         "std_test_score": 0.062287524367393,
         "rank_test_score": 5,
         "_deepnote_index_column": 4
        }
       ]
      },
      "text/plain": "   Unnamed: 0  mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0          52       5.003209      0.118703         0.004350        0.001112   \n1          57      11.609505      0.346065         0.004026        0.001023   \n2          43       8.331176      0.292345         0.003958        0.001316   \n3          20      10.215750      0.353221         0.004155        0.001031   \n4          58       5.581401      0.269410         0.004151        0.001912   \n\n   param_batch_size  param_lr  param_max_epochs param_module__activation_fun  \\\n0               525  0.023882               231                      sigmoid   \n1               304  0.003199               432                         tanh   \n2              1121  0.022958               429                         tanh   \n3              1227  0.006878               441                         tanh   \n4               405  0.003487               244                         tanh   \n\n   param_module__hidden_layers  ...  split3_test_score split4_test_score  \\\n0                            1  ...           0.796305          0.824534   \n1                            2  ...           0.829902          0.799943   \n2                            1  ...           0.816276          0.808714   \n3                            2  ...           0.813090          0.815658   \n4                            1  ...           0.795758          0.811711   \n\n   split5_test_score  split6_test_score  split7_test_score split8_test_score  \\\n0           0.800794           0.884659           0.924938          0.809152   \n1           0.806206           0.881460           0.944821          0.785525   \n2           0.802433           0.869786           0.927757          0.793308   \n3           0.796605           0.853289           0.928002          0.805441   \n4           0.812070           0.888662           0.946810          0.770579   \n\n   split9_test_score  mean_test_score  std_test_score  rank_test_score  \n0           0.981634         0.876031        0.064402                1  \n1           0.969432         0.872926        0.066748                2  \n2           0.907726         0.868457        0.062101                3  \n3           0.943928         0.866816        0.056776                4  \n4           0.923717         0.861921        0.062288                5  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_batch_size</th>\n      <th>param_lr</th>\n      <th>param_max_epochs</th>\n      <th>param_module__activation_fun</th>\n      <th>param_module__hidden_layers</th>\n      <th>...</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>52</td>\n      <td>5.003209</td>\n      <td>0.118703</td>\n      <td>0.004350</td>\n      <td>0.001112</td>\n      <td>525</td>\n      <td>0.023882</td>\n      <td>231</td>\n      <td>sigmoid</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.796305</td>\n      <td>0.824534</td>\n      <td>0.800794</td>\n      <td>0.884659</td>\n      <td>0.924938</td>\n      <td>0.809152</td>\n      <td>0.981634</td>\n      <td>0.876031</td>\n      <td>0.064402</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>57</td>\n      <td>11.609505</td>\n      <td>0.346065</td>\n      <td>0.004026</td>\n      <td>0.001023</td>\n      <td>304</td>\n      <td>0.003199</td>\n      <td>432</td>\n      <td>tanh</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.829902</td>\n      <td>0.799943</td>\n      <td>0.806206</td>\n      <td>0.881460</td>\n      <td>0.944821</td>\n      <td>0.785525</td>\n      <td>0.969432</td>\n      <td>0.872926</td>\n      <td>0.066748</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43</td>\n      <td>8.331176</td>\n      <td>0.292345</td>\n      <td>0.003958</td>\n      <td>0.001316</td>\n      <td>1121</td>\n      <td>0.022958</td>\n      <td>429</td>\n      <td>tanh</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.816276</td>\n      <td>0.808714</td>\n      <td>0.802433</td>\n      <td>0.869786</td>\n      <td>0.927757</td>\n      <td>0.793308</td>\n      <td>0.907726</td>\n      <td>0.868457</td>\n      <td>0.062101</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>10.215750</td>\n      <td>0.353221</td>\n      <td>0.004155</td>\n      <td>0.001031</td>\n      <td>1227</td>\n      <td>0.006878</td>\n      <td>441</td>\n      <td>tanh</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.813090</td>\n      <td>0.815658</td>\n      <td>0.796605</td>\n      <td>0.853289</td>\n      <td>0.928002</td>\n      <td>0.805441</td>\n      <td>0.943928</td>\n      <td>0.866816</td>\n      <td>0.056776</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>58</td>\n      <td>5.581401</td>\n      <td>0.269410</td>\n      <td>0.004151</td>\n      <td>0.001912</td>\n      <td>405</td>\n      <td>0.003487</td>\n      <td>244</td>\n      <td>tanh</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.795758</td>\n      <td>0.811711</td>\n      <td>0.812070</td>\n      <td>0.888662</td>\n      <td>0.946810</td>\n      <td>0.770579</td>\n      <td>0.923717</td>\n      <td>0.861921</td>\n      <td>0.062288</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-f984ff7d-b27c-4f70-9063-55b9ea3a50cb",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "317cce80",
    "execution_start": 1642363438662,
    "execution_millis": 14,
    "deepnote_cell_type": "code"
   },
   "source": "#get the best params for the grid search (cannot use rand_gs.best_estimator directly since it will yield the worst estimator based on MEE)\nbest_params = df_results['params'][0]\nprint(f\"Mean error on the {k}-folds (invMEE) on best model: {df_results['mean_test_score'][0]}\\n\")\nprint(f\"Best params: {best_params}\")",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "Mean error on the 10-folds (invMEE) on best model: 0.8760309958431405\n\nBest params: {'batch_size': 525, 'lr': 0.02388213356411444, 'max_epochs': 231, 'module__activation_fun': 'sigmoid', 'module__hidden_layers': 1, 'module__num_units': 51, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'optimizer__momentum': 0.95, 'optimizer__nesterov': False, 'optimizer__weight_decay': 0.001}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00018-60e7b0eb-43f9-42ba-bf50-66a1b0a8d060",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "16c197cd",
    "execution_start": 1642363990256,
    "execution_millis": 4,
    "deepnote_cell_type": "code"
   },
   "source": "#TODO transform best_param_ into a dict\nBATCH=525\nEPOCHS=231\nLR=0.02388213356411444\nWEIGHT_DECAY = 0.001\nNESTEROV=False\nNUM_UNITS=51\nACTIVATION_FUN='sigmoid'\nOPTIMIZER='SGD'\nHIDDEN_LAYERS=1\nMOMENTUM=0.9\n",
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00024-5481d9c4-dd5f-47fb-81df-ebeaefe5e38b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e82bc267",
    "execution_start": 1642363838010,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "source": "class CupDataset(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n    \ntrain_dataset = CupDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\nval_dataset = CupDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\ntest_dataset = CupDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())",
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00025-5c48b0e2-a6b8-48b3-ab20-4af7f77fef0c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1cc207a6",
    "execution_start": 1642363841763,
    "execution_millis": 8,
    "deepnote_cell_type": "code"
   },
   "source": "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=1)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1)",
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00027-781fd2e1-eee2-46a5-bf91-130ace112e27",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6bad3d92",
    "execution_start": 1642363846114,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00028-19d46bdf-0306-4b07-8e42-1304ae01997d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4e65e027",
    "execution_start": 1642363994490,
    "execution_millis": 5,
    "deepnote_cell_type": "code"
   },
   "source": "model = MyModule(NUM_UNITS, ACTIVATION_FUN, HIDDEN_LAYERS)\nmodel.to(device)\nprint(model)\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=NESTEROV)",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "text": "MyModule(\n  (linear_stack): Sequential(\n    (0): Linear(in_features=10, out_features=51, bias=True)\n    (1): Sigmoid()\n    (2): Linear(in_features=51, out_features=2, bias=True)\n  )\n)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00029-c8feaac3-36d6-4a41-bf16-3592874124a6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d673334b",
    "execution_start": 1642363997343,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "loss_stats = {\n    'train': [],\n    \"val\": []\n}",
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00030-08431223-2e0f-44a1-af6b-b0da9325c1de",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "44dfdda9",
    "execution_start": 1642363853585,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "source": "#function to define the train phase\ndef TrainPhase_Cup(epochs, model, aggregated_losses, aggregated_accuracy, data_train, loss, optimizer):\n    model.train()\n    i = epochs\n\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    for X_batch, y_batch in data_train:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n       \n        y_pred = model(X_batch)\n        #y_pred1 = model(X_batch)\n        \n        single_loss = loss(y_pred, y_batch)\n        #single_loss1 = loss1(y_pred1, y_batch1)\n        #single_loss2 = loss2(y_pred2, y_batch2)\n        #single_loss = torch.add(single_loss1, single_loss2)\n\n        #acc = binary_acc(y_pred, y_batch)\n             \n        single_loss.backward()\n        optimizer.step()\n        \n        epoch_loss += single_loss.item()\n        #epoch_acc += acc.item()\n        \n    aggregated_losses.append(epoch_loss/len(data_train))\n    #aggregated_accuracy.append((epoch_acc/len(data_train))/100)\n    print(f'TRAIN - Epoch {i+0:03}: | Loss: {epoch_loss/len(data_train):.5f}')\n\n    #aggregated_losses.append(single_loss.item())\n    #aggregated_accuracy.append(acc/100)\n    #print(f'TRAIN - Epoch {i+0:03}: | Loss: {single_loss.item():.5f} | Acc: {acc:.3f}')\n    \n    return model, aggregated_losses",
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00031-00aae3d7-f126-4bdc-b4b8-b8ca5a515da9",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f6fcde69",
    "execution_start": 1642363855751,
    "execution_millis": 8,
    "deepnote_cell_type": "code"
   },
   "source": "#function to define the evaluate phase\ndef Evaluate_Cup(epochs, model, aggregated_losses, aggregated_accuracy, data_test, loss_f):\n    \n    i = epochs\n    y_pred_list = []\n    model.eval()\n        \n    epoch_loss = 0\n    epoch_acc = 0\n\n    with torch.no_grad():\n        for X_batch, y_batch in data_test:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n            y_test_pred = model(X_batch)\n            #y_pred1, y_pred2 = model(X_batch)\n\n            loss = loss_f(y_test_pred, y_batch)\n            #loss1 = loss_f1(y_pred1, y_batch1)\n            #loss2 = loss_f2(y_pred2, y_batch2)\n            #loss = torch.add(loss1, loss2)\n            \n            #acc = binary_acc(y_test_pred, y_batch)\n\n            epoch_loss += loss.item()\n            #epoch_loss += loss.item()*X_batch.size(0)\n            #epoch_acc += acc.item()\n\n            y_pred_tag = torch.round(y_test_pred)\n            y_pred_list.append(y_pred_tag.cpu().numpy())\n    \n\n    #y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n\n    aggregated_losses.append(epoch_loss/len(data_test))\n    #aggregated_accuracy.append((epoch_acc/len(data_test))/100)\n    #print(f'TEST - Epoch {i+0:03}: | Loss: {epoch_loss/len(data_test):.5f} | Acc: {epoch_acc/len(data_test):.3f}')\n    print(f'VALIDATION - Epoch {i+0:03}: | Loss: {epoch_loss/len(data_test):.5f}')\n    \n    return aggregated_losses\n    ",
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00032-89fcde0b-d80d-4b7a-aba1-7171c504f9ca",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a1f8240b",
    "execution_start": 1642363857695,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "#function to define the evaluate phase\ndef Result_Cup(model, X_batch):\n    \n    y_pred_list = []\n    model.eval()\n\n    with torch.no_grad():\n        #for X_batch, _ in data_test:\n        X_batch = X_batch.to(device)\n\n        y_test_pred = model(X_batch)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.cpu().numpy()) \n\n    #y_pred_list = [a.squeeze().tolist() for a in y_pred_list]    \n    return y_pred_list\n    ",
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00033-c3ea3100-5c88-412d-90ef-8270bfd27531",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fc7c11b7",
    "execution_start": 1642364002520,
    "execution_millis": 14738,
    "deepnote_cell_type": "code"
   },
   "source": "#prepare empty lists to store accuracy and loss results\nlosses_train = []\naccuracies_train = []\nlosses_test = []\naccuracies_test = []\n\n#train and evaluate for each epoch\nfor epoch in range(1,EPOCHS+1): \n  model, losses_train = TrainPhase_Cup(epoch, model, losses_train, accuracies_train, train_loader, criterion, optimizer)\n  #y_pred_test, losses_test, accuracies_test = Evaluate(epoch, model, losses_test, accuracies_test, test_loader, loss_function)\n  losses_test = Evaluate_Cup(epoch, model, losses_test, accuracies_test, val_loader, mee)\n  \n  print(\"##############################\")",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "text": "TRAIN - Epoch 001: | Loss: 127.37417\nVALIDATION - Epoch 001: | Loss: 9.85895\n##############################\nTRAIN - Epoch 002: | Loss: 108.04677\nVALIDATION - Epoch 002: | Loss: 7.19593\n##############################\nTRAIN - Epoch 003: | Loss: 28.32041\nVALIDATION - Epoch 003: | Loss: 11.01094\n##############################\nTRAIN - Epoch 004: | Loss: 74.48035\nVALIDATION - Epoch 004: | Loss: 8.94185\n##############################\nTRAIN - Epoch 005: | Loss: 27.61208\nVALIDATION - Epoch 005: | Loss: 5.42429\n##############################\nTRAIN - Epoch 006: | Loss: 25.85292\nVALIDATION - Epoch 006: | Loss: 6.94907\n##############################\nTRAIN - Epoch 007: | Loss: 22.57155\nVALIDATION - Epoch 007: | Loss: 2.62374\n##############################\nTRAIN - Epoch 008: | Loss: 5.80046\nVALIDATION - Epoch 008: | Loss: 4.35379\n##############################\nTRAIN - Epoch 009: | Loss: 15.33353\nVALIDATION - Epoch 009: | Loss: 3.20085\n##############################\nTRAIN - Epoch 010: | Loss: 5.33701\nVALIDATION - Epoch 010: | Loss: 3.06293\n##############################\nTRAIN - Epoch 011: | Loss: 7.78557\nVALIDATION - Epoch 011: | Loss: 3.30255\n##############################\nTRAIN - Epoch 012: | Loss: 5.72867\nVALIDATION - Epoch 012: | Loss: 1.90948\n##############################\nTRAIN - Epoch 013: | Loss: 3.83008\nVALIDATION - Epoch 013: | Loss: 2.46646\n##############################\nTRAIN - Epoch 014: | Loss: 4.45154\nVALIDATION - Epoch 014: | Loss: 1.98840\n##############################\nTRAIN - Epoch 015: | Loss: 2.58441\nVALIDATION - Epoch 015: | Loss: 1.77629\n##############################\nTRAIN - Epoch 016: | Loss: 3.51378\nVALIDATION - Epoch 016: | Loss: 1.85891\n##############################\nTRAIN - Epoch 017: | Loss: 2.73099\nVALIDATION - Epoch 017: | Loss: 1.75369\n##############################\nTRAIN - Epoch 018: | Loss: 3.09124\nVALIDATION - Epoch 018: | Loss: 1.85899\n##############################\nTRAIN - Epoch 019: | Loss: 2.35525\nVALIDATION - Epoch 019: | Loss: 1.54398\n##############################\nTRAIN - Epoch 020: | Loss: 2.13465\nVALIDATION - Epoch 020: | Loss: 1.73390\n##############################\nTRAIN - Epoch 021: | Loss: 2.09377\nVALIDATION - Epoch 021: | Loss: 1.56609\n##############################\nTRAIN - Epoch 022: | Loss: 1.70961\nVALIDATION - Epoch 022: | Loss: 1.48022\n##############################\nTRAIN - Epoch 023: | Loss: 2.17162\nVALIDATION - Epoch 023: | Loss: 1.50917\n##############################\nTRAIN - Epoch 024: | Loss: 1.70996\nVALIDATION - Epoch 024: | Loss: 1.40810\n##############################\nTRAIN - Epoch 025: | Loss: 1.93365\nVALIDATION - Epoch 025: | Loss: 1.41802\n##############################\nTRAIN - Epoch 026: | Loss: 1.66271\nVALIDATION - Epoch 026: | Loss: 1.40759\n##############################\nTRAIN - Epoch 027: | Loss: 1.39357\nVALIDATION - Epoch 027: | Loss: 1.38426\n##############################\nTRAIN - Epoch 028: | Loss: 1.45700\nVALIDATION - Epoch 028: | Loss: 1.38666\n##############################\nTRAIN - Epoch 029: | Loss: 1.54843\nVALIDATION - Epoch 029: | Loss: 1.34933\n##############################\nTRAIN - Epoch 030: | Loss: 1.45803\nVALIDATION - Epoch 030: | Loss: 1.32629\n##############################\nTRAIN - Epoch 031: | Loss: 1.46698\nVALIDATION - Epoch 031: | Loss: 1.31559\n##############################\nTRAIN - Epoch 032: | Loss: 1.49188\nVALIDATION - Epoch 032: | Loss: 1.30121\n##############################\nTRAIN - Epoch 033: | Loss: 1.44092\nVALIDATION - Epoch 033: | Loss: 1.30167\n##############################\nTRAIN - Epoch 034: | Loss: 1.70506\nVALIDATION - Epoch 034: | Loss: 1.31659\n##############################\nTRAIN - Epoch 035: | Loss: 1.24807\nVALIDATION - Epoch 035: | Loss: 1.31023\n##############################\nTRAIN - Epoch 036: | Loss: 1.14459\nVALIDATION - Epoch 036: | Loss: 1.28528\n##############################\nTRAIN - Epoch 037: | Loss: 1.33929\nVALIDATION - Epoch 037: | Loss: 1.29325\n##############################\nTRAIN - Epoch 038: | Loss: 1.51819\nVALIDATION - Epoch 038: | Loss: 1.28183\n##############################\nTRAIN - Epoch 039: | Loss: 1.09224\nVALIDATION - Epoch 039: | Loss: 1.26089\n##############################\nTRAIN - Epoch 040: | Loss: 1.09232\nVALIDATION - Epoch 040: | Loss: 1.26727\n##############################\nTRAIN - Epoch 041: | Loss: 1.29454\nVALIDATION - Epoch 041: | Loss: 1.26304\n##############################\nTRAIN - Epoch 042: | Loss: 1.64694\nVALIDATION - Epoch 042: | Loss: 1.26285\n##############################\nTRAIN - Epoch 043: | Loss: 1.64111\nVALIDATION - Epoch 043: | Loss: 1.25094\n##############################\nTRAIN - Epoch 044: | Loss: 1.25205\nVALIDATION - Epoch 044: | Loss: 1.24135\n##############################\nTRAIN - Epoch 045: | Loss: 1.32977\nVALIDATION - Epoch 045: | Loss: 1.26287\n##############################\nTRAIN - Epoch 046: | Loss: 1.28138\nVALIDATION - Epoch 046: | Loss: 1.28676\n##############################\nTRAIN - Epoch 047: | Loss: 1.20897\nVALIDATION - Epoch 047: | Loss: 1.25181\n##############################\nTRAIN - Epoch 048: | Loss: 1.41610\nVALIDATION - Epoch 048: | Loss: 1.24179\n##############################\nTRAIN - Epoch 049: | Loss: 1.31807\nVALIDATION - Epoch 049: | Loss: 1.23896\n##############################\nTRAIN - Epoch 050: | Loss: 1.10680\nVALIDATION - Epoch 050: | Loss: 1.23809\n##############################\nTRAIN - Epoch 051: | Loss: 1.02815\nVALIDATION - Epoch 051: | Loss: 1.26055\n##############################\nTRAIN - Epoch 052: | Loss: 1.13104\nVALIDATION - Epoch 052: | Loss: 1.24895\n##############################\nTRAIN - Epoch 053: | Loss: 1.07237\nVALIDATION - Epoch 053: | Loss: 1.22474\n##############################\nTRAIN - Epoch 054: | Loss: 2.02746\nVALIDATION - Epoch 054: | Loss: 1.22138\n##############################\nTRAIN - Epoch 055: | Loss: 1.24754\nVALIDATION - Epoch 055: | Loss: 1.25684\n##############################\nTRAIN - Epoch 056: | Loss: 1.21799\nVALIDATION - Epoch 056: | Loss: 1.25273\n##############################\nTRAIN - Epoch 057: | Loss: 1.31476\nVALIDATION - Epoch 057: | Loss: 1.23232\n##############################\nTRAIN - Epoch 058: | Loss: 1.73615\nVALIDATION - Epoch 058: | Loss: 1.22145\n##############################\nTRAIN - Epoch 059: | Loss: 1.58703\nVALIDATION - Epoch 059: | Loss: 1.23168\n##############################\nTRAIN - Epoch 060: | Loss: 1.30773\nVALIDATION - Epoch 060: | Loss: 1.23394\n##############################\nTRAIN - Epoch 061: | Loss: 1.21540\nVALIDATION - Epoch 061: | Loss: 1.22140\n##############################\nTRAIN - Epoch 062: | Loss: 1.68828\nVALIDATION - Epoch 062: | Loss: 1.23239\n##############################\nTRAIN - Epoch 063: | Loss: 1.20405\nVALIDATION - Epoch 063: | Loss: 1.24557\n##############################\nTRAIN - Epoch 064: | Loss: 1.00750\nVALIDATION - Epoch 064: | Loss: 1.23083\n##############################\nTRAIN - Epoch 065: | Loss: 1.19836\nVALIDATION - Epoch 065: | Loss: 1.23301\n##############################\nTRAIN - Epoch 066: | Loss: 1.11697\nVALIDATION - Epoch 066: | Loss: 1.23308\n##############################\nTRAIN - Epoch 067: | Loss: 1.03830\nVALIDATION - Epoch 067: | Loss: 1.22040\n##############################\nTRAIN - Epoch 068: | Loss: 1.07502\nVALIDATION - Epoch 068: | Loss: 1.21245\n##############################\nTRAIN - Epoch 069: | Loss: 1.01709\nVALIDATION - Epoch 069: | Loss: 1.21159\n##############################\nTRAIN - Epoch 070: | Loss: 1.11235\nVALIDATION - Epoch 070: | Loss: 1.21179\n##############################\nTRAIN - Epoch 071: | Loss: 1.11154\nVALIDATION - Epoch 071: | Loss: 1.21328\n##############################\nTRAIN - Epoch 072: | Loss: 1.21950\nVALIDATION - Epoch 072: | Loss: 1.19539\n##############################\nTRAIN - Epoch 073: | Loss: 1.35804\nVALIDATION - Epoch 073: | Loss: 1.19579\n##############################\nTRAIN - Epoch 074: | Loss: 1.14322\nVALIDATION - Epoch 074: | Loss: 1.21382\n##############################\nTRAIN - Epoch 075: | Loss: 1.10615\nVALIDATION - Epoch 075: | Loss: 1.22957\n##############################\nTRAIN - Epoch 076: | Loss: 1.28609\nVALIDATION - Epoch 076: | Loss: 1.20598\n##############################\nTRAIN - Epoch 077: | Loss: 1.24385\nVALIDATION - Epoch 077: | Loss: 1.23293\n##############################\nTRAIN - Epoch 078: | Loss: 1.21581\nVALIDATION - Epoch 078: | Loss: 1.21857\n##############################\nTRAIN - Epoch 079: | Loss: 1.99198\nVALIDATION - Epoch 079: | Loss: 1.22891\n##############################\nTRAIN - Epoch 080: | Loss: 1.27172\nVALIDATION - Epoch 080: | Loss: 1.24999\n##############################\nTRAIN - Epoch 081: | Loss: 1.41077\nVALIDATION - Epoch 081: | Loss: 1.21045\n##############################\nTRAIN - Epoch 082: | Loss: 1.18587\nVALIDATION - Epoch 082: | Loss: 1.24336\n##############################\nTRAIN - Epoch 083: | Loss: 1.14583\nVALIDATION - Epoch 083: | Loss: 1.20519\n##############################\nTRAIN - Epoch 084: | Loss: 1.34503\nVALIDATION - Epoch 084: | Loss: 1.24413\n##############################\nTRAIN - Epoch 085: | Loss: 1.14046\nVALIDATION - Epoch 085: | Loss: 1.23227\n##############################\nTRAIN - Epoch 086: | Loss: 1.29792\nVALIDATION - Epoch 086: | Loss: 1.19338\n##############################\nTRAIN - Epoch 087: | Loss: 1.13354\nVALIDATION - Epoch 087: | Loss: 1.21964\n##############################\nTRAIN - Epoch 088: | Loss: 1.28855\nVALIDATION - Epoch 088: | Loss: 1.19165\n##############################\nTRAIN - Epoch 089: | Loss: 1.18853\nVALIDATION - Epoch 089: | Loss: 1.20732\n##############################\nTRAIN - Epoch 090: | Loss: 1.23551\nVALIDATION - Epoch 090: | Loss: 1.21463\n##############################\nTRAIN - Epoch 091: | Loss: 1.03206\nVALIDATION - Epoch 091: | Loss: 1.18628\n##############################\nTRAIN - Epoch 092: | Loss: 1.13865\nVALIDATION - Epoch 092: | Loss: 1.19465\n##############################\nTRAIN - Epoch 093: | Loss: 1.12024\nVALIDATION - Epoch 093: | Loss: 1.18950\n##############################\nTRAIN - Epoch 094: | Loss: 0.98135\nVALIDATION - Epoch 094: | Loss: 1.18951\n##############################\nTRAIN - Epoch 095: | Loss: 1.01663\nVALIDATION - Epoch 095: | Loss: 1.21513\n##############################\nTRAIN - Epoch 096: | Loss: 0.94792\nVALIDATION - Epoch 096: | Loss: 1.17777\n##############################\nTRAIN - Epoch 097: | Loss: 0.94568\nVALIDATION - Epoch 097: | Loss: 1.17697\n##############################\nTRAIN - Epoch 098: | Loss: 1.18149\nVALIDATION - Epoch 098: | Loss: 1.18381\n##############################\nTRAIN - Epoch 099: | Loss: 1.70671\nVALIDATION - Epoch 099: | Loss: 1.16846\n##############################\nTRAIN - Epoch 100: | Loss: 1.73580\nVALIDATION - Epoch 100: | Loss: 1.21273\n##############################\nTRAIN - Epoch 101: | Loss: 1.21049\nVALIDATION - Epoch 101: | Loss: 1.19315\n##############################\nTRAIN - Epoch 102: | Loss: 1.22478\nVALIDATION - Epoch 102: | Loss: 1.17803\n##############################\nTRAIN - Epoch 103: | Loss: 1.08316\nVALIDATION - Epoch 103: | Loss: 1.19801\n##############################\nTRAIN - Epoch 104: | Loss: 1.40799\nVALIDATION - Epoch 104: | Loss: 1.19258\n##############################\nTRAIN - Epoch 105: | Loss: 1.14288\nVALIDATION - Epoch 105: | Loss: 1.21934\n##############################\nTRAIN - Epoch 106: | Loss: 1.07802\nVALIDATION - Epoch 106: | Loss: 1.21590\n##############################\nTRAIN - Epoch 107: | Loss: 0.96606\nVALIDATION - Epoch 107: | Loss: 1.17500\n##############################\nTRAIN - Epoch 108: | Loss: 1.28390\nVALIDATION - Epoch 108: | Loss: 1.19955\n##############################\nTRAIN - Epoch 109: | Loss: 1.13422\nVALIDATION - Epoch 109: | Loss: 1.19952\n##############################\nTRAIN - Epoch 110: | Loss: 1.42939\nVALIDATION - Epoch 110: | Loss: 1.17117\n##############################\nTRAIN - Epoch 111: | Loss: 1.25001\nVALIDATION - Epoch 111: | Loss: 1.18800\n##############################\nTRAIN - Epoch 112: | Loss: 0.99527\nVALIDATION - Epoch 112: | Loss: 1.18958\n##############################\nTRAIN - Epoch 113: | Loss: 1.05182\nVALIDATION - Epoch 113: | Loss: 1.16635\n##############################\nTRAIN - Epoch 114: | Loss: 1.09062\nVALIDATION - Epoch 114: | Loss: 1.18597\n##############################\nTRAIN - Epoch 115: | Loss: 1.26396\nVALIDATION - Epoch 115: | Loss: 1.17139\n##############################\nTRAIN - Epoch 116: | Loss: 1.19540\nVALIDATION - Epoch 116: | Loss: 1.18421\n##############################\nTRAIN - Epoch 117: | Loss: 1.12845\nVALIDATION - Epoch 117: | Loss: 1.20429\n##############################\nTRAIN - Epoch 118: | Loss: 1.16336\nVALIDATION - Epoch 118: | Loss: 1.17080\n##############################\nTRAIN - Epoch 119: | Loss: 0.99511\nVALIDATION - Epoch 119: | Loss: 1.17718\n##############################\nTRAIN - Epoch 120: | Loss: 1.17574\nVALIDATION - Epoch 120: | Loss: 1.16953\n##############################\nTRAIN - Epoch 121: | Loss: 0.94100\nVALIDATION - Epoch 121: | Loss: 1.18489\n##############################\nTRAIN - Epoch 122: | Loss: 1.25903\nVALIDATION - Epoch 122: | Loss: 1.16907\n##############################\nTRAIN - Epoch 123: | Loss: 1.39572\nVALIDATION - Epoch 123: | Loss: 1.20840\n##############################\nTRAIN - Epoch 124: | Loss: 1.13829\nVALIDATION - Epoch 124: | Loss: 1.15816\n##############################\nTRAIN - Epoch 125: | Loss: 1.31801\nVALIDATION - Epoch 125: | Loss: 1.17548\n##############################\nTRAIN - Epoch 126: | Loss: 0.98659\nVALIDATION - Epoch 126: | Loss: 1.15292\n##############################\nTRAIN - Epoch 127: | Loss: 1.41537\nVALIDATION - Epoch 127: | Loss: 1.16806\n##############################\nTRAIN - Epoch 128: | Loss: 1.22686\nVALIDATION - Epoch 128: | Loss: 1.17969\n##############################\nTRAIN - Epoch 129: | Loss: 1.17658\nVALIDATION - Epoch 129: | Loss: 1.19261\n##############################\nTRAIN - Epoch 130: | Loss: 1.45154\nVALIDATION - Epoch 130: | Loss: 1.16516\n##############################\nTRAIN - Epoch 131: | Loss: 1.20694\nVALIDATION - Epoch 131: | Loss: 1.20687\n##############################\nTRAIN - Epoch 132: | Loss: 1.53903\nVALIDATION - Epoch 132: | Loss: 1.18264\n##############################\nTRAIN - Epoch 133: | Loss: 1.38043\nVALIDATION - Epoch 133: | Loss: 1.18335\n##############################\nTRAIN - Epoch 134: | Loss: 1.00535\nVALIDATION - Epoch 134: | Loss: 1.18918\n##############################\nTRAIN - Epoch 135: | Loss: 1.22298\nVALIDATION - Epoch 135: | Loss: 1.16888\n##############################\nTRAIN - Epoch 136: | Loss: 1.40769\nVALIDATION - Epoch 136: | Loss: 1.19939\n##############################\nTRAIN - Epoch 137: | Loss: 1.28836\nVALIDATION - Epoch 137: | Loss: 1.17278\n##############################\nTRAIN - Epoch 138: | Loss: 1.37152\nVALIDATION - Epoch 138: | Loss: 1.19425\n##############################\nTRAIN - Epoch 139: | Loss: 1.41706\nVALIDATION - Epoch 139: | Loss: 1.29394\n##############################\nTRAIN - Epoch 140: | Loss: 1.39237\nVALIDATION - Epoch 140: | Loss: 1.21227\n##############################\nTRAIN - Epoch 141: | Loss: 1.15203\nVALIDATION - Epoch 141: | Loss: 1.20654\n##############################\nTRAIN - Epoch 142: | Loss: 1.55342\nVALIDATION - Epoch 142: | Loss: 1.17795\n##############################\nTRAIN - Epoch 143: | Loss: 1.93225\nVALIDATION - Epoch 143: | Loss: 1.16418\n##############################\nTRAIN - Epoch 144: | Loss: 0.91290\nVALIDATION - Epoch 144: | Loss: 1.20170\n##############################\nTRAIN - Epoch 145: | Loss: 1.30601\nVALIDATION - Epoch 145: | Loss: 1.16926\n##############################\nTRAIN - Epoch 146: | Loss: 1.11080\nVALIDATION - Epoch 146: | Loss: 1.21381\n##############################\nTRAIN - Epoch 147: | Loss: 1.14987\nVALIDATION - Epoch 147: | Loss: 1.18767\n##############################\nTRAIN - Epoch 148: | Loss: 0.91633\nVALIDATION - Epoch 148: | Loss: 1.17955\n##############################\nTRAIN - Epoch 149: | Loss: 1.01684\nVALIDATION - Epoch 149: | Loss: 1.19426\n##############################\nTRAIN - Epoch 150: | Loss: 1.02054\nVALIDATION - Epoch 150: | Loss: 1.16750\n##############################\nTRAIN - Epoch 151: | Loss: 1.21383\nVALIDATION - Epoch 151: | Loss: 1.17953\n##############################\nTRAIN - Epoch 152: | Loss: 1.17326\nVALIDATION - Epoch 152: | Loss: 1.18090\n##############################\nTRAIN - Epoch 153: | Loss: 1.11751\nVALIDATION - Epoch 153: | Loss: 1.15209\n##############################\nTRAIN - Epoch 154: | Loss: 1.31498\nVALIDATION - Epoch 154: | Loss: 1.18640\n##############################\nTRAIN - Epoch 155: | Loss: 0.97656\nVALIDATION - Epoch 155: | Loss: 1.20720\n##############################\nTRAIN - Epoch 156: | Loss: 1.13541\nVALIDATION - Epoch 156: | Loss: 1.16226\n##############################\nTRAIN - Epoch 157: | Loss: 1.15518\nVALIDATION - Epoch 157: | Loss: 1.16648\n##############################\nTRAIN - Epoch 158: | Loss: 1.04223\nVALIDATION - Epoch 158: | Loss: 1.15295\n##############################\nTRAIN - Epoch 159: | Loss: 1.06726\nVALIDATION - Epoch 159: | Loss: 1.16152\n##############################\nTRAIN - Epoch 160: | Loss: 1.09515\nVALIDATION - Epoch 160: | Loss: 1.16662\n##############################\nTRAIN - Epoch 161: | Loss: 1.08915\nVALIDATION - Epoch 161: | Loss: 1.15854\n##############################\nTRAIN - Epoch 162: | Loss: 1.16957\nVALIDATION - Epoch 162: | Loss: 1.16798\n##############################\nTRAIN - Epoch 163: | Loss: 1.09740\nVALIDATION - Epoch 163: | Loss: 1.18028\n##############################\nTRAIN - Epoch 164: | Loss: 0.94348\nVALIDATION - Epoch 164: | Loss: 1.17169\n##############################\nTRAIN - Epoch 165: | Loss: 0.99078\nVALIDATION - Epoch 165: | Loss: 1.14423\n##############################\nTRAIN - Epoch 166: | Loss: 0.94759\nVALIDATION - Epoch 166: | Loss: 1.14907\n##############################\nTRAIN - Epoch 167: | Loss: 1.10545\nVALIDATION - Epoch 167: | Loss: 1.15412\n##############################\nTRAIN - Epoch 168: | Loss: 1.13273\nVALIDATION - Epoch 168: | Loss: 1.14865\n##############################\nTRAIN - Epoch 169: | Loss: 1.04186\nVALIDATION - Epoch 169: | Loss: 1.14844\n##############################\nTRAIN - Epoch 170: | Loss: 1.35870\nVALIDATION - Epoch 170: | Loss: 1.15561\n##############################\nTRAIN - Epoch 171: | Loss: 0.96231\nVALIDATION - Epoch 171: | Loss: 1.14897\n##############################\nTRAIN - Epoch 172: | Loss: 1.29776\nVALIDATION - Epoch 172: | Loss: 1.18400\n##############################\nTRAIN - Epoch 173: | Loss: 1.16391\nVALIDATION - Epoch 173: | Loss: 1.16542\n##############################\nTRAIN - Epoch 174: | Loss: 1.15339\nVALIDATION - Epoch 174: | Loss: 1.16083\n##############################\nTRAIN - Epoch 175: | Loss: 1.43297\nVALIDATION - Epoch 175: | Loss: 1.16386\n##############################\nTRAIN - Epoch 176: | Loss: 1.20173\nVALIDATION - Epoch 176: | Loss: 1.16174\n##############################\nTRAIN - Epoch 177: | Loss: 0.98562\nVALIDATION - Epoch 177: | Loss: 1.15789\n##############################\nTRAIN - Epoch 178: | Loss: 0.90986\nVALIDATION - Epoch 178: | Loss: 1.17621\n##############################\nTRAIN - Epoch 179: | Loss: 1.33979\nVALIDATION - Epoch 179: | Loss: 1.16155\n##############################\nTRAIN - Epoch 180: | Loss: 1.27624\nVALIDATION - Epoch 180: | Loss: 1.17915\n##############################\nTRAIN - Epoch 181: | Loss: 0.99035\nVALIDATION - Epoch 181: | Loss: 1.19032\n##############################\nTRAIN - Epoch 182: | Loss: 1.01153\nVALIDATION - Epoch 182: | Loss: 1.18143\n##############################\nTRAIN - Epoch 183: | Loss: 1.20930\nVALIDATION - Epoch 183: | Loss: 1.16555\n##############################\nTRAIN - Epoch 184: | Loss: 1.10069\nVALIDATION - Epoch 184: | Loss: 1.16566\n##############################\nTRAIN - Epoch 185: | Loss: 0.90667\nVALIDATION - Epoch 185: | Loss: 1.15969\n##############################\nTRAIN - Epoch 186: | Loss: 1.21375\nVALIDATION - Epoch 186: | Loss: 1.16091\n##############################\nTRAIN - Epoch 187: | Loss: 0.95789\nVALIDATION - Epoch 187: | Loss: 1.16696\n##############################\nTRAIN - Epoch 188: | Loss: 1.05444\nVALIDATION - Epoch 188: | Loss: 1.15955\n##############################\nTRAIN - Epoch 189: | Loss: 1.21325\nVALIDATION - Epoch 189: | Loss: 1.15291\n##############################\nTRAIN - Epoch 190: | Loss: 1.06649\nVALIDATION - Epoch 190: | Loss: 1.15386\n##############################\nTRAIN - Epoch 191: | Loss: 0.89993\nVALIDATION - Epoch 191: | Loss: 1.15608\n##############################\nTRAIN - Epoch 192: | Loss: 0.99360\nVALIDATION - Epoch 192: | Loss: 1.15098\n##############################\nTRAIN - Epoch 193: | Loss: 1.15187\nVALIDATION - Epoch 193: | Loss: 1.16214\n##############################\nTRAIN - Epoch 194: | Loss: 0.92605\nVALIDATION - Epoch 194: | Loss: 1.14972\n##############################\nTRAIN - Epoch 195: | Loss: 1.01517\nVALIDATION - Epoch 195: | Loss: 1.14498\n##############################\nTRAIN - Epoch 196: | Loss: 1.07136\nVALIDATION - Epoch 196: | Loss: 1.14810\n##############################\nTRAIN - Epoch 197: | Loss: 1.14007\nVALIDATION - Epoch 197: | Loss: 1.14425\n##############################\nTRAIN - Epoch 198: | Loss: 0.88836\nVALIDATION - Epoch 198: | Loss: 1.16444\n##############################\nTRAIN - Epoch 199: | Loss: 1.32176\nVALIDATION - Epoch 199: | Loss: 1.15845\n##############################\nTRAIN - Epoch 200: | Loss: 1.16871\nVALIDATION - Epoch 200: | Loss: 1.14939\n##############################\nTRAIN - Epoch 201: | Loss: 0.83376\nVALIDATION - Epoch 201: | Loss: 1.15074\n##############################\nTRAIN - Epoch 202: | Loss: 1.17601\nVALIDATION - Epoch 202: | Loss: 1.15756\n##############################\nTRAIN - Epoch 203: | Loss: 1.14126\nVALIDATION - Epoch 203: | Loss: 1.15728\n##############################\nTRAIN - Epoch 204: | Loss: 0.91127\nVALIDATION - Epoch 204: | Loss: 1.14571\n##############################\nTRAIN - Epoch 205: | Loss: 1.21157\nVALIDATION - Epoch 205: | Loss: 1.14909\n##############################\nTRAIN - Epoch 206: | Loss: 0.84378\nVALIDATION - Epoch 206: | Loss: 1.14703\n##############################\nTRAIN - Epoch 207: | Loss: 1.29105\nVALIDATION - Epoch 207: | Loss: 1.14749\n##############################\nTRAIN - Epoch 208: | Loss: 1.02201\nVALIDATION - Epoch 208: | Loss: 1.15162\n##############################\nTRAIN - Epoch 209: | Loss: 0.86910\nVALIDATION - Epoch 209: | Loss: 1.14867\n##############################\nTRAIN - Epoch 210: | Loss: 1.12937\nVALIDATION - Epoch 210: | Loss: 1.15473\n##############################\nTRAIN - Epoch 211: | Loss: 1.13118\nVALIDATION - Epoch 211: | Loss: 1.16823\n##############################\nTRAIN - Epoch 212: | Loss: 1.13322\nVALIDATION - Epoch 212: | Loss: 1.14990\n##############################\nTRAIN - Epoch 213: | Loss: 0.94963\nVALIDATION - Epoch 213: | Loss: 1.18555\n##############################\nTRAIN - Epoch 214: | Loss: 1.26586\nVALIDATION - Epoch 214: | Loss: 1.15856\n##############################\nTRAIN - Epoch 215: | Loss: 0.84284\nVALIDATION - Epoch 215: | Loss: 1.17112\n##############################\nTRAIN - Epoch 216: | Loss: 1.04372\nVALIDATION - Epoch 216: | Loss: 1.18783\n##############################\nTRAIN - Epoch 217: | Loss: 1.07313\nVALIDATION - Epoch 217: | Loss: 1.15716\n##############################\nTRAIN - Epoch 218: | Loss: 0.97803\nVALIDATION - Epoch 218: | Loss: 1.16364\n##############################\nTRAIN - Epoch 219: | Loss: 1.05956\nVALIDATION - Epoch 219: | Loss: 1.17258\n##############################\nTRAIN - Epoch 220: | Loss: 1.06362\nVALIDATION - Epoch 220: | Loss: 1.16330\n##############################\nTRAIN - Epoch 221: | Loss: 0.97226\nVALIDATION - Epoch 221: | Loss: 1.17613\n##############################\nTRAIN - Epoch 222: | Loss: 1.22135\nVALIDATION - Epoch 222: | Loss: 1.15167\n##############################\nTRAIN - Epoch 223: | Loss: 0.97235\nVALIDATION - Epoch 223: | Loss: 1.17505\n##############################\nTRAIN - Epoch 224: | Loss: 1.19471\nVALIDATION - Epoch 224: | Loss: 1.17095\n##############################\nTRAIN - Epoch 225: | Loss: 0.84842\nVALIDATION - Epoch 225: | Loss: 1.19711\n##############################\nTRAIN - Epoch 226: | Loss: 1.00601\nVALIDATION - Epoch 226: | Loss: 1.19014\n##############################\nTRAIN - Epoch 227: | Loss: 0.89695\nVALIDATION - Epoch 227: | Loss: 1.15369\n##############################\nTRAIN - Epoch 228: | Loss: 1.06237\nVALIDATION - Epoch 228: | Loss: 1.14399\n##############################\nTRAIN - Epoch 229: | Loss: 1.03783\nVALIDATION - Epoch 229: | Loss: 1.16255\n##############################\nTRAIN - Epoch 230: | Loss: 1.18599\nVALIDATION - Epoch 230: | Loss: 1.14797\n##############################\nTRAIN - Epoch 231: | Loss: 0.92348\nVALIDATION - Epoch 231: | Loss: 1.14676\n##############################\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00034-0e0bffb4-cee1-4ae8-af4e-8067246393ef",
    "deepnote_output_heights": [
     606
    ],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9593c91",
    "execution_start": 1642364017274,
    "execution_millis": 284,
    "deepnote_cell_type": "code"
   },
   "source": "#create a loss plot\nplt.figure(figsize=(15,10))\nplt.plot(losses_train,'-')\n\nplt.plot(losses_test,'-.')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train','Test'])\nplt.title('Train vs Test Losses')\n \nplt.show()",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1080x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 891,
       "height": 604
      }
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d9d840c9-75b3-4185-bb01-105f5cca8d83' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "10b26e99-167c-44c5-82d9-a804fbe3ae77",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 }
}